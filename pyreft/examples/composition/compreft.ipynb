{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334d633f-6f52-4d71-a2ed-83f56a5257d9",
   "metadata": {},
   "source": [
    "### Can we do compositional ReFT?\n",
    "\n",
    "I have:\n",
    "\n",
    "- **A ReFT for continuing sentences in German**\n",
    "- **A ReFT for following instructions**\n",
    "\n",
    "Can I just combine them and have an \"instruction-following model that speaks German\"? Let's see!\n",
    "\n",
    "First of all, you need to know the notations of **subspace**, **linear subspace**, and **orthonormal linear subspaces**! You can read more about these in Atticus's [causal abstraction paper](https://arxiv.org/abs/2301.04709). Briefly, here is what they are:\n",
    "\n",
    "- **subspace**: you can think of it as a single dimension of an NN's representation in the NN's original basis (learned one).\n",
    "- **linear subspace**: representation in a changed basis, and the new basis is a linear combination (i.e., any rotation) of the original basis.\n",
    "- **orthonormal linear subspaces**: if the new linear subspace is produced by an orthonormal projection, then each dimension (or sub-subspace, sorry about the confusion here) in that new basis is orthogonal to each other. Or more strictly speaking, *it maintains the orthogonality if the original basis has it*.\n",
    "\n",
    "So for ReFT, we can theoretically leverage the notation of subspace, and train different subspaces for different tasks separately, and snap them together at the inference time! Let's see if it will work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98347df-766a-4887-a2a0-c8ad195ca944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    ReftSupervisedDataset,\n",
    "    LoreftIntervention\n",
    ")\n",
    "\n",
    "prompt_no_input_template = \"\"\"Below is an instruction that \\\n",
    "describes a task. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "transformers.set_seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f19971-698c-49c9-8f6b-60170595949f",
   "metadata": {},
   "source": [
    "### Loading the base LM (LLaMA-1 here! not Llama-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a908ccf-9873-437f-9726-3573fbb18fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359fd882d3c4f3dabe1c26d0def0eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "# load model (take 1 min)\n",
    "model_name_or_path = \"yahma/llama-7b-hf\" # yahma/llama-7b-hf or yahma/llama-13b-hf\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "model_max_length = 512\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb70475-7411-48e9-afe1-c49a771f0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Subspace partitions:\n",
    "\n",
    "# Let's have a LoReFT of rank 8, and assign\n",
    "# - the first 4 rank to german sentence completion\n",
    "# - the next 4 rank to instruction following\n",
    "##################################################\n",
    "HELLASWAG_SUBSPACE = [0,1,2,3]\n",
    "INSTRUCT_SUBSPACE = [4,5,6,7]\n",
    "\n",
    "def preprocess_hellaswag_de_for_reft(examples):\n",
    "    label = int(examples[\"label\"])\n",
    "    if len(examples[\"endings_de\"]) < 4:\n",
    "        output = examples[\"endings_de\"][-1]\n",
    "    else:\n",
    "        output = examples[\"endings_de\"][label]\n",
    "    examples[\"instruction\"] = examples[\"ctx\"]\n",
    "    examples[\"output\"] = output\n",
    "    examples[\"subspaces\"] = HELLASWAG_SUBSPACE\n",
    "    return examples\n",
    "\n",
    "def preprocess_ultrafeedback_for_reft(examples):\n",
    "    examples[\"subspaces\"] = INSTRUCT_SUBSPACE\n",
    "    examples[\"output\"] += tokenizer.eos_token\n",
    "    return examples\n",
    "\n",
    "raw_dataset = load_dataset(\"LeoLM/HellaSwag_de\")\n",
    "drop_features = list(raw_dataset[\"train\"].features.keys())\n",
    "raw_dataset = raw_dataset.map(preprocess_hellaswag_de_for_reft)\n",
    "hellaswag_de_dataset = raw_dataset.remove_columns(drop_features)[\"train\"]\n",
    "\n",
    "raw_dataset = load_dataset(\"json\", data_files=\"./ultrafeedback_1k.json\")[\"train\"]\n",
    "raw_dataset = raw_dataset.map(preprocess_ultrafeedback_for_reft)\n",
    "ultrafeedback_dataset = raw_dataset.remove_columns([\"input\"])\n",
    "\n",
    "subspace_dataset = concatenate_datasets([hellaswag_de_dataset, ultrafeedback_dataset])\n",
    "\n",
    "class SubloreftIntervention(LoreftIntervention):\n",
    "    \"\"\"\n",
    "    This is a LoReFT that supports subspace interventions!\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self, base, source=None, subspaces=None\n",
    "    ):\n",
    "        assert subspaces is not None\n",
    "        output = []\n",
    "        \n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        diff = self.act_fn(self.learned_source(base)) - rotated_base\n",
    "        \n",
    "        batched_subspace = []\n",
    "        batched_weights = []\n",
    "        \n",
    "        for example_i in range(len(subspaces)):\n",
    "            LHS = (diff[example_i, :, subspaces[example_i]])\n",
    "            RHS = self.rotate_layer.weight[..., subspaces[example_i]].T\n",
    "            # print(diff.shape, LHS.shape, RHS.shape, base.shape, subspaces)\n",
    "            batched_subspace += [LHS]\n",
    "            batched_weights += [RHS]\n",
    "\n",
    "        batched_subspace = torch.stack(batched_subspace, dim=0)\n",
    "        batched_weights = torch.stack(batched_weights, dim=0)\n",
    "        output = base + torch.bmm(batched_subspace, batched_weights)\n",
    "\n",
    "        return self.dropout(output.to(base.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5d36e-4a3b-4532-a392-4874b4c5b87a",
   "metadata": {},
   "source": [
    "### Load rank 8 LoReFT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12ba835-ffad-4db2-bbde-9ac8225df204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 65,544 || trainable model params: 0\n",
      "model params: 6,738,415,616 || trainable%: 0.0009726915603776257\n"
     ]
    }
   ],
   "source": [
    "TARGET_LAYER = 15\n",
    "\n",
    "# get reft model\n",
    "reft_config = ReftConfig(representations={\n",
    "    \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "    \"intervention\": SubloreftIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=8)})\n",
    "reft_model = get_reft_model(model, reft_config)\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9119a-4e1a-4b38-8bef-b332c1611199",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Note that in total, we only have **2,000 training examples**, since LoReFT works with low resource settings - a bonus we did not fully explore in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e227fe-c451-4f39-954f-27dc3f7509c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2000/2000 [00:03<00:00, 573.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ReftSupervisedDataset(\n",
    "    \"Subloreft\", None, tokenizer, dataset=subspace_dataset,\n",
    "    **{\"num_interventions\": 1, \"position\": \"l1\", \"share_weights\": False},\n",
    "    input_field=\"input\", instruction_field=\"instruction\", output_field=\"output\",\n",
    "    no_stop=True\n",
    ")\n",
    "data_collator_fn = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "data_collator = ReftDataCollator(data_collator=data_collator_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b2ad0-1187-441d-934d-12b477f94898",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "Note that we are not training a shared subspace for two tasks! We are training them individually by providing the `subspaces` field in the input! Checkout [pyvene](https://github.com/stanfordnlp/pyvene) about how to use `subspaces` field - there are other stuff we haven't tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d0e9f8-b5ea-434f-a1aa-94991a0f68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/nlp/scr/peterwz/miniconda3/envs/pyreft-comp/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 17:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.181700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './tmp/checkpoint-500/intervenable_model' already exists.\n",
      "Directory './tmp/checkpoint-750/intervenable_model' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.224351099650065, metrics={'train_runtime': 1022.4403, 'train_samples_per_second': 5.868, 'train_steps_per_second': 0.734, 'total_flos': 0.0, 'train_loss': 1.224351099650065, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=3.0, output_dir=\"./tmp\", learning_rate=5e-3, report_to=[],\n",
    "    per_device_train_batch_size=8, logging_steps=50\n",
    ")\n",
    "trainer = ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, \n",
    "    train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5a459-fc9b-49e0-9d3e-ca6f725211fd",
   "metadata": {},
   "source": [
    "### Interact with the German sentence completion subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99046802-ec47-4ed4-9538-8602b2fef244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How to keep a healthy lifestyle?\n",
      "\n",
      "### Response:\n",
      ", die gesundheitliche Ernährung ist wichtig für alle Menschen. Die Ernährungsempfehlungen der Deutschen Gesellschaft für Ernährung (DGE) sind in einem Buch veröffentlicht worden. Das Buch enthält viele Tipps und Informationen über Ernährung und Gesundheit. Es gibt auch eine Liste von Ernährungsempfängern, die sich mit Ernährung befassen.\n",
      "\n",
      "## Einzelnachweise\n",
      "\n",
      "1.  http://www.dge-ev.de/index.php?id=20&L=1\n"
     ]
    }
   ],
   "source": [
    "instruction = \"How to keep a healthy lifestyle?\"\n",
    "\n",
    "prompt = prompt_no_input_template % instruction\n",
    "prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    subspaces=[[HELLASWAG_SUBSPACE]],\n",
    "    intervene_on_prompt=True, max_new_tokens=128, do_sample=False, \n",
    "    no_repeat_ngram_size=5, repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0642761-0039-4b11-915b-818f82f18eb3",
   "metadata": {},
   "source": [
    "### Interact with the instruction following subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e560061a-067b-4994-96ee-204d6ac0abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How to keep a healthy lifestyle?\n",
      "\n",
      "### Response:\n",
      "To maintain a healthy lifesytle, it's important to eat nutritious foods and get enough exercise. It's also essential to drink plenty of water and get enough sleep. These are some ways to stay healthy.\n"
     ]
    }
   ],
   "source": [
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    subspaces=[[INSTRUCT_SUBSPACE]],\n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, \n",
    "    no_repeat_ngram_size=5, repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ea777-9c34-43a7-9737-78e0d682ef34",
   "metadata": {},
   "source": [
    "### Interact with both subspaces, partially!\n",
    "\n",
    "To interact with both of them, you can simply change the `subspaces` field at the inference time to any combinations you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c5ff5be-27bc-4ea4-b476-fc7d9c38c33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How to keep a healthy lifestyle?\n",
      "\n",
      "### Response:\n",
      "Jugendliche können eine gesunde Lebensweise durch verschiedene Aktivitäten erreichen, wie zum Beispiel Sport und Spiele. Sie können auch ein gesundes Essen und einen guten Schlaf haben. Außerdem kann man sich um die Gesundheit der anderen Menschen kümmern, indem man ihnen hilft oder sie unterstützt.\n",
      "\n",
      "Es ist wichtig, dass Jugendliche eine gesunde Ernährung aufnehmen, um ihre Körper zu stärken und gesund zu bleiben. Es gibt viele Möglichkeiten, wie man eine gesunde Ernähhung aufnimmt, wie zum Beispiel frische Gemüse, Obst und Fleisch. Auch kann man sich um den Verzehr von Süßigkeiten und Zuckerhaltigen Produkten kümmern.\n",
      "\n",
      "Ein weiterer Aspekt für eine gesunde Ernahrung ist, dass man genügend Flüssigkeit aufnimmt, um seinen Körper zu hydrieren. Wasser ist das beste Fluid, das man aufnehmen kann, da es keine Fettanteile enthält.\n",
      "\n",
      "Um einen gesunden Schlaf zu bekommen, sollte man vor dem Schlafengehen ruhig sein und sich nicht überlasten. Des Weiteren sollte man sich regelmäßig ausruhen und genügend Zeit für Ruhe finden.\n",
      "\n",
      "Außerdem kann man sich um andere Menschen kümmert, indem man sie unterstützt oder hilft. So kann man beispielsweise bei einem Krankheitsfall oder einer Notlage helfen.\n",
      "\n",
      "Insgesamt kann man eine gesunde Lebensführung durch verschiedene Aktionen erreichen, wie Sport, Spielen, Essen, Schlafen und Hilfe anderer Menschen.\n"
     ]
    }
   ],
   "source": [
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    # sometimes, leaving subspaces [4,5] out will lead to a better performance\n",
    "    subspaces=[[[0,1,2,3,4,5,6,7]]], \n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, \n",
    "    no_repeat_ngram_size=5, repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b03c37-f2b8-47a8-a93e-32430a41e052",
   "metadata": {},
   "source": [
    "This is an early sneak-peek of our **Feature Compartmentalization**, and **Schematic ReFT**! Stay tuned, and explore with us! We think there are basically infinite number of causal pathways in the neural network waiting for us to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9456fd3-eb27-4ddf-99a8-f8c4c96568ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
